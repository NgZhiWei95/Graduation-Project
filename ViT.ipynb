{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzHUZdkqOxE3",
        "outputId": "d7079199-8e23-4ffb-937d-4f7de21bb8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U tensorflow-addons"
      ],
      "metadata": {
        "id": "ipKh-xT-PBRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38f5fe52-cb01-47c6-9824-3d995711ff37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 15.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports and libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import pandas as pd\n",
        "import os\n",
        "import cv2\n",
        "import seaborn as sn\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "fkbCKhIhjZcw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Directory of train and test folders\n",
        "train_path = \"/content/drive/MyDrive/SignatureDataSet/training\"\n",
        "test_path = \"/content/drive/MyDrive/SignatureDataSet/testing\""
      ],
      "metadata": {
        "id": "U5MM817grVWz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pass train and test images into list and resize to 200 by 200\n",
        "x_train = []\n",
        "for folder in os.listdir(train_path):\n",
        "    sub_path = train_path+\"/\"+folder\n",
        "    for img in os.listdir(sub_path):\n",
        "        image_path = sub_path+\"/\"+img\n",
        "        img_arr = cv2.imread(image_path)\n",
        "        img_arr = cv2.resize(img_arr,(200,200))\n",
        "        x_train.append(img_arr)\n",
        "\n",
        "x_test = []\n",
        "for folder in os.listdir(test_path):\n",
        "    sub_path = test_path+\"/\"+folder\n",
        "    for img in os.listdir(sub_path):\n",
        "        image_path = sub_path+\"/\"+img\n",
        "        img_arr = cv2.imread(image_path)\n",
        "        img_arr = cv2.resize(img_arr,(200,200))\n",
        "        x_test.append(img_arr)"
      ],
      "metadata": {
        "id": "t1vHuGdMre_Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "train_x = np.array(x_train)\n",
        "test_x = np.array(x_test)"
      ],
      "metadata": {
        "id": "53JnztZLrsI6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "train_x = train_x/255.0\n",
        "test_x = test_x/255.0"
      ],
      "metadata": {
        "id": "BitFgM_8rwGs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Rescaling\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "metadata": {
        "id": "lac9X-Ivr2yx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Store train and test folder into variables, size 200 by 200, 2 classes\n",
        "training_set = train_datagen.flow_from_directory(train_path, target_size = (200, 200), batch_size = 10, class_mode = 'binary')\n",
        "\n",
        "test_set = test_datagen.flow_from_directory(test_path, target_size = (200, 200), batch_size = 10, class_mode = 'binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV6TmcZPr9H7",
        "outputId": "68ba9d58-8420-4299-dea9-3c28d293aa42"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1920 images belonging to 2 classes.\n",
            "Found 60 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = training_set.classes\n",
        "test_y = test_set.classes"
      ],
      "metadata": {
        "id": "MnKhXweosJx1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters used for ViT\n",
        "num_classes = 2\n",
        "input_shape = (200, 200, 3)\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "image_size = 200\n",
        "patch_size = 6\n",
        "num_patches = (image_size//patch_size)**2\n",
        "projection_dim = 64\n",
        "num_heads = 4\n",
        "transformer_units = [projection_dim*2, projection_dim]\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [2048, 1024]"
      ],
      "metadata": {
        "id": "Bn2t6uA5sxe7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization and resizing\n",
        "data_augmentation = keras.Sequential(\n",
        "    [layers.Normalization(),\n",
        "    layers.Resizing(image_size, image_size)],\n",
        "    name = \"data_augmentation\",\n",
        ")\n",
        "data_augmentation.layers[0].adapt(train_x)"
      ],
      "metadata": {
        "id": "WCP1tbWhtKiS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multilayer perceptron\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "0-PZVLUQtVRg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create patches as layers\n",
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images = images,\n",
        "            sizes = [1, self.patch_size, self.patch_size, 1],\n",
        "            strides = [1, self.patch_size, self.patch_size, 1],\n",
        "            rates = [1, 1, 1, 1],\n",
        "            padding = \"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "metadata": {
        "id": "1mg-QuKctbC9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Patch encoding layer\n",
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim = num_patches, output_dim = projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "sP4IJRAvteIe"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ViT model\n",
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    augmented = data_augmentation(inputs)\n",
        "    patches = Patches(patch_size)(augmented)\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    for _ in range(transformer_layers):\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ],
      "metadata": {
        "id": "ZeW1_5RctgeS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile, train, and evaluate model with test dataset\n",
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    model.compile(optimizer=optimizer, loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")])\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_filepath, monitor=\"val_accuracy\", save_best_only=True, save_weights_only=True)\n",
        "\n",
        "    history = model.fit(x=train_x, y=train_y, batch_size=batch_size, epochs=num_epochs, validation_split=0.2, callbacks=[checkpoint_callback])\n",
        "\n",
        "    model.load_weights(checkpoint_filepath)\n",
        "    _, accuracy = model.evaluate(test_x, test_y)\n",
        "    print(f\"Overall test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "vit_classifier = create_vit_classifier()\n",
        "history = run_experiment(vit_classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKIKrDjNson8",
        "outputId": "22d2a09d-dd4c-4473-cbc7-140d6fd9ec5e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "154/154 [==============================] - 2970s 19s/step - loss: 16.0066 - accuracy: 0.5378 - val_loss: 3.4466 - val_accuracy: 0.1875\n",
            "Epoch 2/10\n",
            "154/154 [==============================] - 3088s 20s/step - loss: 1.4102 - accuracy: 0.6022 - val_loss: 1.2103 - val_accuracy: 0.0234\n",
            "Epoch 3/10\n",
            "154/154 [==============================] - 3753s 24s/step - loss: 0.8279 - accuracy: 0.6087 - val_loss: 1.0450 - val_accuracy: 0.0495\n",
            "Epoch 4/10\n",
            "154/154 [==============================] - 3452s 22s/step - loss: 0.7013 - accuracy: 0.6374 - val_loss: 0.6455 - val_accuracy: 0.7344\n",
            "Epoch 5/10\n",
            "154/154 [==============================] - 3479s 23s/step - loss: 0.6309 - accuracy: 0.6680 - val_loss: 1.0955 - val_accuracy: 0.0286\n",
            "Epoch 6/10\n",
            "154/154 [==============================] - 3407s 22s/step - loss: 0.5778 - accuracy: 0.7012 - val_loss: 1.3437 - val_accuracy: 0.1172\n",
            "Epoch 7/10\n",
            "154/154 [==============================] - 3448s 22s/step - loss: 0.5497 - accuracy: 0.7298 - val_loss: 1.0809 - val_accuracy: 0.2240\n",
            "Epoch 8/10\n",
            "154/154 [==============================] - 3671s 24s/step - loss: 0.4648 - accuracy: 0.7799 - val_loss: 0.9791 - val_accuracy: 0.4193\n",
            "Epoch 9/10\n",
            "154/154 [==============================] - 3690s 24s/step - loss: 0.3844 - accuracy: 0.8255 - val_loss: 1.6086 - val_accuracy: 0.3151\n",
            "Epoch 10/10\n",
            "154/154 [==============================] - 3753s 24s/step - loss: 0.3407 - accuracy: 0.8535 - val_loss: 3.0759 - val_accuracy: 0.0755\n",
            "2/2 [==============================] - 41s 21s/step - loss: 0.6955 - accuracy: 0.5000\n",
            "Overall test accuracy: 50.0%\n"
          ]
        }
      ]
    }
  ]
}